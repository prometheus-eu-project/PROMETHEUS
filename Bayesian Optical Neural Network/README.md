# ğŸ§  Bayesian Optical Neural Network

This project implements a **Bayesian Optical Neural Network (BONN)** in Python. It simulates photonic neural networks using **Clements mesh interferometers** and trains them using **Bayesian inference**, **regularized learning**, or **classical gradient descent**.

---

## ğŸš€ Features

- ğŸŒ€ Optical neural network using Clements mesh topology  
- ğŸ” Support for complex-valued forward/backward propagation  
- ğŸ§® Custom loss functions (Cross Entropy, MSE)  
- ğŸ“ˆ Training via:
  - Bayesian "Bits-Back" method
  - L2 Regularization
  - Classical Adjoint Optimization
- ğŸ“Š Dataset support: **Iris** and **MNIST**
- ğŸ”§ Simulates fabrication errors, quantization, thermal crosstalk

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-username/bayesian-optical-neural-network.git
cd bayesian-optical-neural-network
pip install -r requirements.txt
```

### ğŸ§© Dependencies

- `numpy`
- `scipy`
- `matplotlib`
- `torch`
- `sklearn`
- `torchvision`
- `tqdm`

---

## ğŸ§ª Usage

The main script is `main.py`. It uses the **Iris dataset** by default:

```bash
python main.py
```

Youâ€™ll see training loss and accuracy plots for 1000 epochs. Final test accuracy is printed at the end.

---

## ğŸ§  Training Methods

You can train using one of three approaches:
- `method='bits-back'` â€“ full Bayesian training using KL divergence
- `method='regularization'` â€“ L2 regularized loss
- `method='classical'` â€“ deterministic training without uncertainty modeling

Set this in `main.py`:
```python
method = 'bits-back'  # or 'regularization' / 'classical'
```

---

## ğŸ“š Datasets

### Iris (default)
- 4 input features
- 3 classes (setosa, versicolor, virginica)

### MNIST (optional)
To use MNIST, replace `prepare_iris()` with `prepare_mnist(freq_radius=5)`.

---

## ğŸ“Š Output

- `losses`: Training loss per epoch
- `accuracies`: Training accuracy per epoch
- Final test accuracy is displayed
- Optional: Save and visualize trained phase shifters

---

## ğŸ›  Project Structure

```
â”œâ”€â”€ main.py             # Entry point
â”œâ”€â”€ bayesian.py         # Loss functions, KL divergence, Adam optimizer
â”œâ”€â”€ photonics.py        # Optical layers, activation functions, training logic
â”œâ”€â”€ problems.py         # Dataset loaders (Iris, MNIST)
â””â”€â”€ README.md           # Project documentation
```

---

## ğŸ§  Background

This implementation simulates optical computing by modeling how photonic phase shifters can implement neural networks. Using the **Adjoint Method**, it performs backpropagation by measuring optical powers â€” a technique aligned with current photonic hardware research.

---

## ğŸ¤ Contributing

PRs welcome! Please include:
- Clear descriptions of the change
- Tests or reproducible results where possible

---

## ğŸ“„ License

MIT License Â© 2025
